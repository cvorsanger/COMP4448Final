{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center style=\"font-size:48px;\">Further Clean Up</center>\n",
    "<br>\n",
    "<center>EDA on the relationships in the dataset between csvs</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library and Data Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Geospatial\n",
    "import geopandas as gpd\n",
    "\n",
    "# Plotting\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MousePosition, HeatMapWithTime\n",
    "\n",
    "# Warning Control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('../Data/car-assignments.csv')\n",
    "cc = pd.read_csv('../Data/cc_data.csv', encoding='cp1252', parse_dates=['timestamp'])\n",
    "gps = pd.read_csv('../Data/gps.csv', parse_dates=['Timestamp'])\n",
    "loyalty = pd.read_csv('../Data/loyalty_data.csv', encoding='cp1252', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-csv EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPS &amp; Car Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from timestamp and merge two dataframes\n",
    "locations = gps.merge(cars, left_on='id', right_on='CarID', how='left')\n",
    "locations.drop(columns='CarID', inplace =True)\n",
    "timeUnit = ['weekend', 'hour']\n",
    "for unit in timeUnit:\n",
    "    if unit == 'hour':\n",
    "        locations[unit] = locations['Timestamp'].apply(lambda x: x.hour)\n",
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap that is segmented by employment type\n",
    "m = folium.Map(location=[36.07, 24.87], zoom_start=14)\n",
    "kronos = gpd.read_file('../Geospatial/Kronos_Island.shp')\n",
    "folium.features.GeoJson(kronos,  style_function= lambda feature: {'fillColor' : 'black'}, control=False).add_to(m)\n",
    "abila = gpd.read_file('../Geospatial/Abila.shp')\n",
    "folium.features.GeoJson(abila, style_function= lambda feature: {'color' : 'white'}, control=False).add_to(m)\n",
    "\n",
    "for job in locations.CurrentEmploymentType.unique():\n",
    "    if job is not np.nan:\n",
    "        if job == 'Facilities':\n",
    "            temp = locations[(locations.CurrentEmploymentType == job) | (locations.LastName.isnull())]\n",
    "            g = folium.FeatureGroup(name = job)\n",
    "        else:\n",
    "            temp = locations[locations.CurrentEmploymentType == job]\n",
    "            g = folium.FeatureGroup(name = job, show=False)\n",
    "        m.add_child(g)\n",
    "        HeatMap(temp[['lat', 'long']], radius =20).add_to(g)\n",
    "fmtr = \"function(num) {return L.Util.formatNum(num, 3) + ' ยบ ';};\"\n",
    "MousePosition(position='topleft', separator=' | ', prefix=\"Mouse:\",\n",
    "              lat_formatter=fmtr, lng_formatter=fmtr).add_to(m)\n",
    "folium.LayerControl(collapsed=False).add_to(m)\n",
    "m.save('../images/SegmentedHeatmap.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are definatly difference in movement between job types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are cars located over the entire dataset\n",
    "for job in locations.CurrentEmploymentType.unique():\n",
    "    if job is not np.nan:\n",
    "        m = folium.Map(location=[36.07, 24.87], zoom_start=14)\n",
    "        kronos = gpd.read_file('../Geospatial/Kronos_Island.shp')\n",
    "        folium.features.GeoJson(kronos,  style_function= lambda feature: {'fillColor' : 'black'}, control=False).add_to(m)\n",
    "        abila = gpd.read_file('../Geospatial/Abila.shp')\n",
    "        folium.features.GeoJson(abila, style_function= lambda feature: {'color' : 'white'}, control=False).add_to(m)\n",
    "        weights = []\n",
    "        index = []\n",
    "        if job == 'Facilities':\n",
    "            temp = locations[(locations.CurrentEmploymentType == job) | (locations.LastName.isnull())]\n",
    "            for hour in sorted(temp.hour.unique()):\n",
    "                weights.append(temp[temp.hour == hour][['lat', 'long']].groupby(['lat', 'long']).count().reset_index().values.tolist())\n",
    "                index.append(pd.to_datetime(hour, format='%H').strftime('%I %p'))\n",
    "                [[ x.append(0.1) for x in y] for y in weights]\n",
    "        else:\n",
    "            temp = locations[locations.CurrentEmploymentType == job]\n",
    "            for hour in sorted(temp.hour.unique()):\n",
    "                weights.append(temp[temp.hour == hour][['lat', 'long']].groupby(['lat', 'long']).count().reset_index().values.tolist())\n",
    "                index.append(pd.to_datetime(hour, format='%H').strftime('%I %p'))\n",
    "                [[ x.append(0.1) for x in y] for y in weights]\n",
    "        HeatMapWithTime(weights, index = index).add_to(m)\n",
    "        fmtr = \"function(num) {return L.Util.formatNum(num, 3) + ' ยบ ';};\"\n",
    "        MousePosition(position='topleft', separator=' | ', prefix=\"Mouse:\",\n",
    "                    lat_formatter=fmtr, lng_formatter=fmtr).add_to(m)\n",
    "        print(job)\n",
    "        m.save(\"../images/{}.html\".format(job))\n",
    "        display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are differences in the typical day between job types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card &amp; Loyalty Purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes and extract the time elements\n",
    "cc['Is_Loyalty'] = False\n",
    "loyalty['Is_Loyalty'] = True\n",
    "buys = pd.concat([cc, loyalty])\n",
    "buys = buys.merge(cars, left_on=['LastName', 'FirstName'], right_on=['LastName', 'FirstName'])\n",
    "timeUnit = ['day', 'hour', 'minute', 'second']\n",
    "for unit in timeUnit:\n",
    "    if unit == 'day':\n",
    "        buys[unit] = buys['timestamp'].apply(lambda x: x.day)\n",
    "    if unit == 'hour':\n",
    "        buys[unit] = buys['timestamp'].apply(lambda x: x.hour)\n",
    "    if unit == 'minute':\n",
    "        buys[unit] = buys['timestamp'].apply(lambda x: x.minute)\n",
    "    if unit == 'second':\n",
    "        buys[unit] = buys['timestamp'].apply(lambda x: x.second)\n",
    "buys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see if there are any locations that are only in one dataframe\n",
    "uniqueCC = set(buys[~buys.Is_Loyalty]['location']).difference(set(buys[buys.Is_Loyalty]['location']))\n",
    "print('Unique locations in the credit card dataframe : {}'.format(uniqueCC))\n",
    "uniqueLoyalty = set(buys[buys.Is_Loyalty]['location']).difference(set(buys[~buys.Is_Loyalty]['location']))\n",
    "print('Unique locations in the loyalty dataframe : {}'.format(uniqueLoyalty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of duplicate charges\n",
    "buys[['CarID', 'location', 'day']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a ton of duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the duplicate transactions for one person\n",
    "buys[buys[['CarID', 'location', 'day']].duplicated(keep = False)][buys.FirstName == 'Willem'].sort_values(['location', 'day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the duplicate transactions are ones that are in both the loyalty and credit card dataframes. Also seems like the credit card prices have some suspect entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = buys.sort_values('Is_Loyalty')\n",
    "onlyLoyalty = len(s[s[['CarID', 'location', 'day', 'price']].duplicated(keep='first')][buys.Is_Loyalty])/len(buys[buys.Is_Loyalty]) * 100\n",
    "onlyCC = len(buys[buys[['CarID', 'location', 'day', 'price']].duplicated(keep = 'last')][~buys.Is_Loyalty])/len(buys[~buys.Is_Loyalty]) * 100\n",
    "print('~{} % of transactions in the loyalty transactions were in the credit card transactions'.format(round(onlyLoyalty,3)) )\n",
    "print('~{} % of transactions in the credit card transactions were in the loyalty transactions'.format(round(onlyCC,3)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buys.groupby('CurrentEmploymentType')[['price']].agg([np.median, 'mean']).sort_values(('price', 'median'), ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facilities tend to spend much more. This could be due to large purhases for vehicle maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buys.groupby('CurrentEmploymentTitle')[['price']].agg([np.median, 'mean']).sort_values(('price', 'median'), ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buys.groupby(['CurrentEmploymentType','Is_Loyalty'])[['price']].agg([np.median, 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buys.groupby(['CurrentEmploymentTitle','Is_Loyalty'])[['price']].agg([np.median, 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span  style=\"width:600px;display:inline-block;text-align:left\">\n",
    "        <a href=\"./EDA.ipynb\">&#60;&#60;Exploratoy Data Analysis</a>\n",
    "    </span>\n",
    "    <span style=\"width:600px;display:inline-block;text-align:right\">\n",
    "        <a href=\"./CleanUp.ipynb\">Data Clean Up&#62;&#62;</a>\n",
    "    </span>\n",
    "</div>\n",
    "<div>\n",
    "    <center>\n",
    "        <span style=\"width:200px;display:inline-block;text-align:center\">\n",
    "            <a href=\"./Master.ipynb\">Master Notebook</a>\n",
    "        </span>\n",
    "        <span style=\"width:200px;display:inline-block;text-align:center\">\n",
    "            <a href=\"../README.md\">Table of Contents</a>\n",
    "        </span>\n",
    "    </center>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbc5d808ed13306b281beb3b148e89003940376baf375861433a5e62b978e9c3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('COMP4449')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
